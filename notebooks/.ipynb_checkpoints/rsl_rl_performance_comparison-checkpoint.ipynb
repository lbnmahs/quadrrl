{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RSL-RL Policy Performance Comparison\n",
        "\n",
        "This notebook performs comprehensive performance comparisons between different trained policies:\n",
        "- Flat vs Rough terrain\n",
        "- Different robot platforms (Anymal-C, Anymal-D, Unitree Go2)\n",
        "- Direct locomotion vs Manager-based navigation\n",
        "\n",
        "All comparisons focus on the latest checkpoints from training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries and utilities\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add scripts directory to path\n",
        "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"scripts\"))\n",
        "\n",
        "# Import analysis utilities\n",
        "from rsl_rl_analysis_utils import (\n",
        "    COMPARISONS, KEY_METRICS, TASK_METRICS,\n",
        "    load_all_metrics, extract_key_metrics,\n",
        "    plot_comparison_bar, plot_training_curves, plot_metric_heatmap,\n",
        "    TENSORBOARD_AVAILABLE\n",
        ")\n",
        "\n",
        "# Define directories\n",
        "LOGS_DIR = PROJECT_ROOT / \"logs\" / \"rsl_rl\"\n",
        "OUTPUT_DIR = Path.cwd() / \"comparison_results\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Logs directory: {LOGS_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"TensorBoard available: {TENSORBOARD_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load All Metrics\n",
        "\n",
        "Load training metrics from all TensorBoard log files for the runs defined in the comparison groups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all metrics\n",
        "print(\"Loading metrics from TensorBoard logs...\")\n",
        "all_metrics = load_all_metrics(LOGS_DIR)\n",
        "\n",
        "print(f\"\\nTotal runs loaded: {len(all_metrics)}\")\n",
        "print(\"\\nLoaded runs:\")\n",
        "for run_key, run_data in all_metrics.items():\n",
        "    num_metrics = len(run_data['metrics'])\n",
        "    print(f\"  - {run_data['display_name']}: {num_metrics} metrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract key metrics into DataFrame\n",
        "metrics_df = extract_key_metrics(all_metrics)\n",
        "\n",
        "print(f\"Metrics DataFrame shape: {metrics_df.shape}\")\n",
        "print(f\"\\nAvailable columns:\")\n",
        "print(list(metrics_df.columns))\n",
        "\n",
        "# Display summary\n",
        "display(metrics_df[['display_name', 'experiment', 'category']].head())\n",
        "\n",
        "# Save to CSV\n",
        "metrics_csv = OUTPUT_DIR / \"metrics_summary.csv\"\n",
        "metrics_df.to_csv(metrics_csv, index=False)\n",
        "print(f\"\\nMetrics summary saved to: {metrics_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Comparison: Flat vs Rough Terrain\n",
        "\n",
        "Compare performance on flat vs rough terrain across different robots and tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Total reward comparison\n",
        "metric_col = 'Reward___Total_reward_mean'\n",
        "if metric_col in metrics_df.columns:\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    plot_comparison_bar(\n",
        "        metrics_df,\n",
        "        'flat_vs_rough',\n",
        "        metric_col,\n",
        "        title='Total Reward: Flat vs Rough Terrain',\n",
        "        ylabel='Average Episode Reward',\n",
        "        ax=ax\n",
        "    )\n",
        "    plt.savefig(OUTPUT_DIR / \"flat_vs_rough_reward.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"Metric {metric_col} not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training curves for flat vs rough\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "plot_training_curves(\n",
        "    all_metrics,\n",
        "    'flat_vs_rough',\n",
        "    'Reward / Total reward (mean)',\n",
        "    title='Training Progress: Total Reward (Flat vs Rough)',\n",
        "    smoothing=10,\n",
        "    ax=ax\n",
        ")\n",
        "plt.savefig(OUTPUT_DIR / \"flat_vs_rough_training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comparison: Robot Platforms (Rough Terrain)\n",
        "\n",
        "Compare different robot platforms on rough terrain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Robot comparison on rough terrain\n",
        "metric_col = 'Reward___Total_reward_mean'\n",
        "if metric_col in metrics_df.columns:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    plot_comparison_bar(\n",
        "        metrics_df,\n",
        "        'robot_comparison_rough',\n",
        "        metric_col,\n",
        "        title='Total Reward: Robot Platform Comparison (Rough Terrain)',\n",
        "        ylabel='Average Episode Reward',\n",
        "        ax=ax\n",
        "    )\n",
        "    plt.savefig(OUTPUT_DIR / \"robot_comparison_rough_reward.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training curves for robot comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "plot_training_curves(\n",
        "    all_metrics,\n",
        "    'robot_comparison_rough',\n",
        "    'Reward / Total reward (mean)',\n",
        "    title='Training Progress: Robot Platform Comparison (Rough)',\n",
        "    smoothing=10,\n",
        "    ax=ax\n",
        ")\n",
        "plt.savefig(OUTPUT_DIR / \"robot_comparison_rough_training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comparison: Direct vs Manager-Based Control\n",
        "\n",
        "Compare direct locomotion control vs manager-based navigation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Direct vs Manager-based comparison\n",
        "metric_col = 'Reward___Total_reward_mean'\n",
        "if metric_col in metrics_df.columns:\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    plot_comparison_bar(\n",
        "        metrics_df,\n",
        "        'direct_vs_manager',\n",
        "        metric_col,\n",
        "        title='Total Reward: Direct vs Manager-Based Control',\n",
        "        ylabel='Average Episode Reward',\n",
        "        ax=ax\n",
        "    )\n",
        "    plt.savefig(OUTPUT_DIR / \"direct_vs_manager_reward.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comparison: Navigation - Flat vs Rough\n",
        "\n",
        "Compare navigation task performance on flat vs rough terrain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Navigation terrain comparison\n",
        "metric_col = 'Reward___Total_reward_mean'\n",
        "if metric_col in metrics_df.columns:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    plot_comparison_bar(\n",
        "        metrics_df,\n",
        "        'navigation_terrain',\n",
        "        metric_col,\n",
        "        title='Navigation: Total Reward (Flat vs Rough)',\n",
        "        ylabel='Average Episode Reward',\n",
        "        ax=ax\n",
        "    )\n",
        "    plt.savefig(OUTPUT_DIR / \"navigation_terrain_reward.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Position tracking (navigation-specific)\n",
        "position_col = 'Info___Episode_Reward_position_tracking'\n",
        "if position_col in metrics_df.columns:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    plot_comparison_bar(\n",
        "        metrics_df,\n",
        "        'navigation_terrain',\n",
        "        position_col,\n",
        "        title='Navigation: Position Tracking Accuracy (Flat vs Rough)',\n",
        "        ylabel='Position Tracking Score',\n",
        "        ax=ax\n",
        "    )\n",
        "    plt.savefig(OUTPUT_DIR / \"navigation_terrain_position_tracking.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comprehensive Metric Heatmaps\n",
        "\n",
        "Heatmaps showing multiple metrics across different runs for easy comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive heatmaps\n",
        "key_metric_cols = [\n",
        "    'Reward___Total_reward_mean',\n",
        "    'Reward___Instantaneous_reward_mean',\n",
        "    'Episode___Total_timesteps_mean',\n",
        "    'Loss___Policy_loss',\n",
        "    'Loss___Value_loss',\n",
        "]\n",
        "\n",
        "# Flat vs Rough heatmap\n",
        "plot_metric_heatmap(\n",
        "    metrics_df,\n",
        "    'flat_vs_rough',\n",
        "    key_metric_cols,\n",
        "    title=\"Comprehensive Metrics: Flat vs Rough Terrain\"\n",
        ")\n",
        "plt.savefig(OUTPUT_DIR / \"heatmap_flat_vs_rough.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Robot comparison heatmap\n",
        "plot_metric_heatmap(\n",
        "    metrics_df,\n",
        "    'robot_comparison_rough',\n",
        "    key_metric_cols,\n",
        "    title=\"Comprehensive Metrics: Robot Platform Comparison (Rough)\"\n",
        ")\n",
        "plt.savefig(OUTPUT_DIR / \"heatmap_robot_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistical Summary and Insights\n",
        "\n",
        "Generate statistical summaries and key insights from the comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary by category\n",
        "print(\"=\"*80)\n",
        "print(\"STATISTICAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_cols = ['Reward___Total_reward_mean', 'Episode___Total_timesteps_mean']\n",
        "available_summary_cols = [c for c in summary_cols if c in metrics_df.columns]\n",
        "\n",
        "if available_summary_cols:\n",
        "    print(\"\\nSummary by Category:\")\n",
        "    category_summary = metrics_df.groupby('category')[available_summary_cols].agg(['mean', 'std', 'min', 'max'])\n",
        "    display(category_summary)\n",
        "    \n",
        "    # Full metrics table\n",
        "    print(\"\\nFull Metrics Table (Latest Checkpoint Values):\")\n",
        "    display_cols = ['display_name', 'experiment', 'category'] + available_summary_cols\n",
        "    full_table = metrics_df[display_cols].sort_values(by='Reward___Total_reward_mean', ascending=False)\n",
        "    display(full_table)\n",
        "    \n",
        "    # Save summary\n",
        "    full_table.to_csv(OUTPUT_DIR / \"full_metrics_summary.csv\", index=False)\n",
        "    print(f\"\\nFull summary saved to: {OUTPUT_DIR / 'full_metrics_summary.csv'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate key insights\n",
        "print(\"=\"*80)\n",
        "print(\"KEY INSIGHTS AND FINDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "insights = []\n",
        "\n",
        "# 1. Flat vs Rough performance\n",
        "flat_runs = metrics_df[metrics_df['category'] == 'flat']\n",
        "rough_runs = metrics_df[metrics_df['category'] == 'rough']\n",
        "\n",
        "if 'Reward___Total_reward_mean' in metrics_df.columns:\n",
        "    flat_mean = flat_runs['Reward___Total_reward_mean'].mean()\n",
        "    rough_mean = rough_runs['Reward___Total_reward_mean'].mean()\n",
        "    \n",
        "    if pd.notna(flat_mean) and pd.notna(rough_mean):\n",
        "        diff_pct = ((rough_mean - flat_mean) / abs(flat_mean)) * 100 if flat_mean != 0 else 0\n",
        "        insights.append({\n",
        "            'Finding': 'Terrain Difficulty Impact',\n",
        "            'Detail': f'Average reward on rough terrain is {diff_pct:.1f}% {'higher' if diff_pct > 0 else 'lower'} than flat terrain',\n",
        "            'Flat Mean': f\"{flat_mean:.3f}\",\n",
        "            'Rough Mean': f\"{rough_mean:.3f}\"\n",
        "        })\n",
        "\n",
        "# 2. Best performing policy\n",
        "if 'Reward___Total_reward_mean' in metrics_df.columns:\n",
        "    best_run = metrics_df.loc[metrics_df['Reward___Total_reward_mean'].idxmax()]\n",
        "    insights.append({\n",
        "        'Finding': 'Best Performing Policy',\n",
        "        'Detail': f\"{best_run['display_name']} achieves the highest total reward: {best_run['Reward___Total_reward_mean']:.3f}\",\n",
        "        'Experiment': best_run['experiment']\n",
        "    })\n",
        "\n",
        "# 3. Training stability\n",
        "if 'Episode___Total_timesteps_mean' in metrics_df.columns:\n",
        "    stable_runs = metrics_df[metrics_df['Episode___Total_timesteps_mean'] > metrics_df['Episode___Total_timesteps_mean'].quantile(0.75)]\n",
        "    insights.append({\n",
        "        'Finding': 'Training Stability',\n",
        "        'Detail': f\"{len(stable_runs)} policies show high episode stability (top quartile)\",\n",
        "        'Top Stable Policies': ', '.join(stable_runs['display_name'].tolist()[:5])\n",
        "    })\n",
        "\n",
        "# Print insights\n",
        "for i, insight in enumerate(insights, 1):\n",
        "    print(f\"\\n{i}. {insight['Finding']}\")\n",
        "    print(f\"   {insight['Detail']}\")\n",
        "    for key, value in insight.items():\n",
        "        if key not in ['Finding', 'Detail']:\n",
        "            print(f\"   {key}: {value}\")\n",
        "\n",
        "# Save insights\n",
        "insights_df = pd.DataFrame(insights)\n",
        "insights_df.to_csv(OUTPUT_DIR / \"insights.csv\", index=False)\n",
        "print(f\"\\n\\nInsights saved to: {OUTPUT_DIR / 'insights.csv'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has generated comprehensive performance comparisons including:\n",
        "- Bar charts comparing key metrics across different configurations\n",
        "- Training curves showing learning progress\n",
        "- Heatmaps for multi-metric comparisons\n",
        "- Statistical summaries and key insights\n",
        "\n",
        "All visualizations have been saved to the `comparison_results/` directory.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
